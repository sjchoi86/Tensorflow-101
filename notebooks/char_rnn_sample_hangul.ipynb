{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Hangul RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Imported\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Import Packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import string\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import hgtk\n",
    "\n",
    "from six.moves import cPickle\n",
    "from TextLoader import *\n",
    "\n",
    "print (\"Packages Imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset using TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "type of 'data_loader' is <class 'dict'>, length is 76\n",
      "\n",
      "\n",
      "data_loader.vocab looks like \n",
      "{'ㅀ': 51, '7': 71, 'ㅟ': 35, 'ㅓ': 11, 'ㅣ': 6, '-': 65, 'ㄷ': 13, 'ㅈ': 15, 'ㅚ': 33, '2': 62, '9': 64, 'ㅒ': 74, 'ㅐ': 21, '?': 41, ')': 46, \"'\": 49, 'ㅡ': 8, 'ㅇ': 2, 'ㄴ': 4, 'ㅛ': 38, 'ㅌ': 30, 'ㄶ': 44, '3': 66, '8': 68, '_': 69, 'ㅖ': 39, 'ㄺ': 48, ':': 57, '\"': 28, 'ㅘ': 26, 'ㄼ': 54, 'ㅏ': 3, 'ㅄ': 43, 'ㄻ': 55, '5': 63, 'ㅆ': 22, 'ㅞ': 61, 'ㅠ': 37, 'ㄸ': 32, '1': 58, 'ㄿ': 56, 'ㅂ': 17, 'ㅅ': 10, ' ': 1, 'ㅔ': 20, 'ㄱ': 7, 'ㅑ': 34, 'ᴥ': 0, '4': 67, 'ㄲ': 29, '>': 75, 'ㅜ': 16, '0': 73, 'ㅎ': 14, 'ㅗ': 9, 'ㄵ': 50, 'ㅉ': 40, 'ㅢ': 25, 'ㅙ': 53, '!': 52, 'ㄾ': 70, '.': 24, 'ㅃ': 47, '\\n': 19, ',': 27, 'ㅝ': 36, 'ㅊ': 23, '6': 59, 'ㄹ': 5, 'ㄳ': 60, '(': 45, 'ㅕ': 18, 'ㅋ': 42, 'ㅍ': 31, '\\x1a': 72, 'ㅁ': 12} \n",
      "\n",
      "\n",
      "type of 'data_loader.chars' is <class 'tuple'>, length is 76\n",
      "\n",
      "\n",
      "data_loader.chars looks like \n",
      "('ᴥ', ' ', 'ㅇ', 'ㅏ', 'ㄴ', 'ㄹ', 'ㅣ', 'ㄱ', 'ㅡ', 'ㅗ', 'ㅅ', 'ㅓ', 'ㅁ', 'ㄷ', 'ㅎ', 'ㅈ', 'ㅜ', 'ㅂ', 'ㅕ', '\\n', 'ㅔ', 'ㅐ', 'ㅆ', 'ㅊ', '.', 'ㅢ', 'ㅘ', ',', '\"', 'ㄲ', 'ㅌ', 'ㅍ', 'ㄸ', 'ㅚ', 'ㅑ', 'ㅟ', 'ㅝ', 'ㅠ', 'ㅛ', 'ㅖ', 'ㅉ', '?', 'ㅋ', 'ㅄ', 'ㄶ', '(', ')', 'ㅃ', 'ㄺ', \"'\", 'ㄵ', 'ㅀ', '!', 'ㅙ', 'ㄼ', 'ㄻ', 'ㄿ', ':', '1', '6', 'ㄳ', 'ㅞ', '2', '5', '9', '-', '3', '4', '8', '_', 'ㄾ', '7', '\\x1a', '0', 'ㅒ', '>') \n"
     ]
    }
   ],
   "source": [
    "data_dir    = \"data/nine_dreams\"\n",
    "batch_size  = 50\n",
    "seq_length  = 50\n",
    "data_loader = TextLoader(data_dir, batch_size, seq_length)\n",
    "# This makes \"vocab.pkl\" and \"data.npy\" in \"data/nine_dreams\"   \n",
    "#  from \"data/nine_dreams/input.txt\" \n",
    "vocab_size = data_loader.vocab_size\n",
    "vocab = data_loader.vocab\n",
    "chars = data_loader.chars\n",
    "print ( \"type of 'data_loader' is %s, length is %d\" \n",
    "       % (type(data_loader.vocab), len(data_loader.vocab)) )\n",
    "print ( \"\\n\" )\n",
    "print (\"data_loader.vocab looks like \\n%s \" %\n",
    "       (data_loader.vocab))\n",
    "print ( \"\\n\" )\n",
    "print ( \"type of 'data_loader.chars' is %s, length is %d\" \n",
    "       % (type(data_loader.chars), len(data_loader.chars)) )\n",
    "print ( \"\\n\" )\n",
    "print (\"data_loader.chars looks like \\n%s \" % (data_loader.chars,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Ready\n"
     ]
    }
   ],
   "source": [
    "rnn_size   = 512\n",
    "num_layers = 3\n",
    "grad_clip  = 5.\n",
    "\n",
    "_batch_size = 1\n",
    "_seq_length = 1\n",
    "\n",
    "vocab_size = data_loader.vocab_size\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    # Select RNN Cell\n",
    "    unitcell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([unitcell] * num_layers)\n",
    "    # Set paths to the graph \n",
    "    input_data = tf.placeholder(tf.int32, [_batch_size, _seq_length])\n",
    "    targets    = tf.placeholder(tf.int32, [_batch_size, _seq_length])\n",
    "    initial_state = cell.zero_state(_batch_size, tf.float32)\n",
    "\n",
    "    # Set Network\n",
    "    with tf.variable_scope('rnnlm'):\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "            inputs = tf.split(tf.nn.embedding_lookup(embedding, input_data), _seq_length, 1)\n",
    "            inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "            \n",
    "    # Loop function for seq2seq\n",
    "    def loop(prev, _):\n",
    "        prev = tf.nn.xw_plus_b(prev, softmax_w, softmax_b)\n",
    "        prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "        return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "    # Output of RNN \n",
    "    outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state\n",
    "                                , cell, loop_function=None, scope='rnnlm')\n",
    "    output = tf.reshape(tf.concat(outputs, 1), [-1, rnn_size])\n",
    "    logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "    # Next word probability \n",
    "    probs = tf.nn.softmax(logits)\n",
    "    # Define LOSS\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], # Input\n",
    "        [tf.reshape(targets, [-1])], # Target\n",
    "        [tf.ones([_batch_size * _seq_length])], # Weight \n",
    "        vocab_size)\n",
    "    # Define Optimizer\n",
    "    cost = tf.reduce_sum(loss) / _batch_size / _seq_length\n",
    "    final_state = last_state\n",
    "    lr = tf.Variable(0.0, trainable=False)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    _optm = tf.train.AdamOptimizer(lr)\n",
    "    optm = _optm.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "print (\"Network Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling function done.\n"
     ]
    }
   ],
   "source": [
    "# Sample ! \n",
    "def sample( sess, chars, vocab, __probs, num=200, prime=u'ㅇㅗᴥㄴㅡㄹᴥ '):\n",
    "    state = sess.run(cell.zero_state(1, tf.float32))\n",
    "    _probs = __probs\n",
    "    prime = list(prime)\n",
    "    for char in prime[:-1]:\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {input_data: x, initial_state:state}\n",
    "        [state] = sess.run([final_state], feed)\n",
    "\n",
    "    def weighted_pick(weights):\n",
    "        weights = weights / np.sum(weights) \n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "    ret = prime\n",
    "    char = prime[-1]\n",
    "    for n in range(num):\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {input_data: x, initial_state:state}\n",
    "        [_probsval, state] = sess.run([_probs, final_state], feed)\n",
    "        p = _probsval[0]\n",
    "        sample = int(np.random.choice(len(p), p=p))\n",
    "        # sample = weighted_pick(p)\n",
    "        # sample = np.argmax(p)\n",
    "        pred = chars[sample]\n",
    "        ret += pred\n",
    "        char = pred\n",
    "    return ret\n",
    "print (\"sampling function done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "누\n",
      "구\n",
      " \n",
      "Prime Text : 누구  => ㄴㅜᴥㄱㅜᴥ \n",
      "data/nine_dreams/model.ckpt-99000\n",
      "SAMPLED TEXT = ㄴㅜᴥㄱㅜᴥ ㅅㅓㅇᴥㄳㅐㅅᴥㅇㅡㄹᴥ ㅇㅣᴥㅁㅣᴥ ㅁㅗㅅᴥㅎㅏㄹᴥ ㅎㅘㅇᴥㅅㅏㄴᴥㅇㅡᴥㄹㅗᴥ ㅊㅗᴥㅁㅕㄴᴥㅎㅏᴥㄴㅣᴥ, ㅇㅣᴥㅇㅔᴥ ㅅㅜㅁᴥㅇㅡㄹᴥ \n",
      "ㄱㅏㅂᴥㄱㅗᴥ, ㄱㅣᴥㄷㅓᴥㄲㅔᴥ ㅇㅏᴥㄹㅚᴥㄱㅣᴥㄹㅡㄹᴥ, \n",
      "  \"ㅇㅝㄹᴥㅇㅘㅇᴥㅇㅣᴥ ㄷㅐᴥㄷㅏㅂᴥㅎㅏᴥㄷㅚᴥ\n",
      "  \"ㄴㅏㅇᴥㅈㅏᴥ ㅇㅣᴥ ㅅㅗᴥㅈㅓᴥㄱㅏᴥ ㄷㅐㄱᴥ ㄸㅓᴥㅇㅓᴥ ㅈㅏㅇᴥㅊㅓㅂᴥㅇㅡㄹᴥ ㄴㅐᴥㄹㅣᴥㅅㅕㅆᴥㅇㅡᴥㄴㅣᴥ ㅈㅔᴥㄱㅗㅇᴥㅇㅣᴥㅇㅗᴥ?\"\n",
      "  \"ㅇㅕㅇᴥㅇㅑㅇᴥㄱㅗㅇᴥㅈㅜᴥㄱㅏᴥ ㅅㅗᴥㅁㅐᴥㄱㅏᴥ ㄷㅏᴥㄱㅗᴥㅎㅏᴥㅇㅕᴥ ㅇㅌᴥㅇㅜᴥㄴㅏᴥ ㅇㅕㅁᴥㄹㅏᴥㄷㅏᴥㅇㅘᴥ ㅇㅗᴥㄴㅡㄹᴥㅂㅓㅂᴥㄱㅗᴥ ㄴㅏㅁᴥㄱㅗᴥ ㅇㅓㅄᴥㄴㅏᴥㅇㅣᴥㄲㅏᴥ?\"\n",
      "  ㅅㅓㅁᴥㅇㅝㄹᴥㅇㅣᴥ ㄷㅏᴥㅅㅣᴥ ㅎㅏᴥㅇㅕᴥㄱㅡㅁᴥㄲㅔᴥ ㄷㅜᴥㄹㅣᴥㄱㅏᴥ ㅌㅡㄹᴥㄹㅣㅁᴥㅇㅣᴥ ㅅㅣㅁᴥㅂㅏㅇᴥㅎㅏㄴᴥ ㅇㅏᴥㅂㅕㄹᴥㅍㅜㄴᴥㅇㅡㄹᴥ ㅅㅏᴥㄹㅡㄹᴥ ㄸㅏᴥㄹㅡᴥㄷㅗᴥㄹㅏᴥ. \n",
      "ㄸㅗᴥㅎㅏㄴᴥ ㅅㅣㄴᴥㄱㅗㅇᴥㅁㅕㅇᴥㅇㅡㅁᴥ, ㅈㅣㄱᴥㄴㅕᴥㄱㅏᴥ ㄱㅡᴥㄷㅐᴥㅈㅘᴥㅇㅘᴥ ㅁㅓㄷᴥㄱㅗᴥ ㅈㅔᴥ ㅂㅜᴥㅁㅗᴥㅈㅣᴥ ㄴㅗㅅᴥㅎㅏᴥㅇㅕㅆᴥㄷㅏᴥ. \n",
      "\n",
      "    ㄱㅗㄱᴥㅈㅓㄱᴥㅇㅢᴥ ㅈㅓㄴᴥㄱㅜㄱᴥ(ㅇㅏㄴᴥㅈㅔᴥ(ㅅㅏㅁᴥㅂㅗㄱᴥ, \n",
      " ㅅㅣㄴᴥㅅㅓㅇᴥㄷㅗᴥ ㅎㅗㄴᴥㅇㅕㄴᴥㅇㅡㄴᴥ ㅅㅓㄱᴥㅂㅜㄴᴥㅊㅜᴥㅅㅓㅇᴥㅇㅣㄹᴥ ㄴㅏㅇᴥㅈㅏᴥㄱㅏᴥ ㅇㅓㅄᴥㄴㅡㄴᴥ ㅁㅏㄹᴥㅇㅣᴥㄹㅗᴥㄷㅏᴥ.\"\n",
      "  ㅇㅣᴥㅇㅔᴥ ㅊㅜㄴᴥㅇㅜㄴᴥㅇㅣᴥ ㅇㅣᴥㄹㅡᴥㄱㅣᴥㄹㅡㄹᴥ\n",
      "  'ㄴㅓᴥㅎㅢᴥㄹㅡㄹᴥ ㄴㅐᴥㄱㅕᴥ ㅇㅗㄷᴥㅈㅏㄴᴥ ㄱㅗㅇᴥㅈㅜᴥㄱㅏᴥ ㄷㅡㄹᴥㅇㅓᴥㄱㅏᴥㄱㅔᴥ ㅎㅏㄹᴥ \n",
      "ㄱㅓㅅᴥㅇㅣᴥㄹㅣᴥㅇㅛᴥ. ㅅㅏㅇᴥㅅㅓᴥㄲㅔᴥㅅㅓᴥ ㅈㅗᴥㅊㅓㅇᴥㅇㅡㄹᴥ ㄴㅏᴥㄱㅔᴥ ㅎㅏᴥㄷㅓᴥㄹㅏᴥ. \n",
      "  \"ㅅㅐᴥㅅㅗᴥ ㄱㅖᴥㄹㅏㄴᴥㅎㅣᴥ ㄷㅡㄹᴥㅇㅓᴥㄱㅏᴥ ㄱㅖᴥㄹㅏㅇᴥㅇㅔᴥㄱㅔᴥ ㄴㅐᴥㅁㅜᴥㄹㅡㄹᴥ ㅉㅔᴥㅇㅓᴥ ㅈㅗᴥㅎㅏᴥㄱㅗᴥ ㅇㅣㅆᴥㅇㅡㅁᴥㄱㅘᴥ ㅇㅓᴥㅉㅣᴥ ㅅㅓㄹᴥㅇㅣㄴᴥㅈㅡㅌᴥㄱㅔᴥ ㅁㅏㄹᴥㅆㅡㅁᴥㅎㅐㅆᴥㄷㅓᴥㄹㅏᴥ. \n",
      "  ㄱㅓᴥㅁㅜㄴᴥㄱㅗᴥㅇㅢᴥ ㅈㅏᴥㅈㅜᴥ ㄷㅗᴥㅂㅓㄴᴥㅇㅣᴥ ㄷㅗㄹᴥㅇㅣᴥㄹㅗᴥ ㄱㅟᴥㄹㅗㄴᴥㅎㅏᴥㄴㅡᴥㄴㅑᴥ? ㄴㅓㅀᴥㅇㅓᴥ ㄱㅜㄴᴥㅅㅏᴥㅇㅢᴥ ㅅㅗㄱᴥㅇㅣᴥㅅㅣᴥㅇㅓㅆᴥㄴㅏᴥㅇㅣᴥㄷㅏᴥ.\"\n",
      "  ㅎㅏㄴᴥㄹㅣㅁᴥㅇㅣᴥ ㅅㅏㄹᴥㅅㅓㄴᴥㅊㅣᴥㄹㅡㄹᴥ ㅅㅏㅁᴥㄱㅣᴥㄱㅗᴥㄴㅡㄴᴥ ㄱㅓㅅᴥㄷㅡㄹᴥㅇㅣᴥㄱㅗᴥ, ㅍㅜㄹᴥㅅㅏᴥㅂㅚᴥㄹㅗᴥ ㅎㅏㅁᴥㅇㅏᴥㄹㅗᴥㅅㅓᴥ ㅇㅛㄴᴥㅌㅣㅁᴥㅎㅏᴥㄹㅏᴥㄴㅏᴥㄴㅣᴥ ㅁㅓㄹᴥㄹㅣᴥ ㅅㅏㅇᴥㄱㅗㅇᴥㄲㅔᴥㅅㅓᴥ ㅇㅣᴥㅇㅔᴥ ㅇㅟᴥㅇㅓㅊᴥ(ㄱㅏㄴᴥㅅㅓㅇᴥㅎㅏᴥㅇㅗᴥㅁㅐᴥ ㅅㅡㄹᴥㅎㅏᴥㄹㅡㄹᴥ ㅎㅏᴥㄱㅣᴥ \n",
      "ㅇㅓㅄᴥㄱㅔㅆᴥㄱㅏᴥ ㅇㅏᴥㄹㅣᴥㄷㅓㄼᴥㅇㅔᴥㄱㅔᴥ ㄲㅜㄺᴥㅇㅓᴥㄴㅏᴥㄹㅏᴥ ㅎㅏㅁᴥㅈㅜᴥㅈㅣᴥ ㅇㅏㄶᴥㅇㅡㅁᴥㅇㅣᴥ ㄷㅏᴥ ㅅㅓᴥㅂㅗᴥㄷㅐㄱᴥ \n",
      "ㅈㅔᴥㅅㅏᴥㄴㅡㄴᴥ ㅇㅏㄴᴥㄷㅡㅅᴥ(ㅅㅗㄴᴥㅈㅣㄹᴥㅇㅢᴥ ㅈㅣᴥㅂㅜㄱᴥ ㅇㅛㅌᴥㄱㅘᴥ ㄴㅏㄴᴥㅇㅑㅇᴥㄱㅜㄱᴥㅈㅜᴥㄲㅔᴥㅅㅓᴥ ㅎㅚᴥㅂㅗㄱᴥ \n",
      "ㅂㅜㄹᴥㅂㅏᴥㅇㅔᴥ ㅅㅔᴥㅅㅏㅇᴥㅎㅏᴥㅁㅕᴥ ㅅㅗᴥㅇㅠᴥㄴㅡㄴᴥ ㅂㅏᴥㅇㅗㅂᴥㄴㅏᴥㅇㅣᴥㄷㅏᴥ. ㄴㅐᴥ ㅇㅓᴥㅅㅣㅅᴥㅅㅏㅂᴥ ㄱㅕㅇᴥㅇㅣᴥ ㄷㅏᴥㄸㅏㄴᴥㅎㅐㅆᴥㄷㅏᴥ. \n",
      "  ㄱㅖᴥㅅㅓㄱᴥㅇㅢᴥ ㅅㅏᴥㅇㅠᴥㅇㅣㅁᴥㅇㅣᴥ ㅂㅜㄴᴥㅂㅗᴥㄷㅏᴥ ㅅㅐㅇᴥㄱㅟᴥㅎㅏᴥㄱㅗᴥ ㅅㅗㄱᴥㅇㅔᴥ ㄸㅏᴥㄹㅏㅆᴥㅇㅡᴥㄴㅣᴥ ㄱㅡᴥ ㄸㅡㅅᴥㅇㅡㄴᴥ ㄸㅏㄹᴥㅇㅣᴥㅇㅓㅆᴥㄷㅏᴥ. ㅅㅗᴥㅇㅠᴥㄴㅡㄴᴥ ㅎㅗㄱᴥㅅㅣᴥ ㅂㅜᴥㄹㅡᴥㅅㅣᴥㄴㅣᴥ ㄱㅡᴥ ㅅㅗㄱᴥㅅㅔᴥ\n",
      "  ㅅㅓㅇᴥㄱㅜㅇᴥㅇㅡㄹᴥ ㅁㅏㄹᴥㅎㅏᴥㄴㅡㄴᴥ ㅅㅣᴥㄴㅐᴥㄹㅡㄹᴥ ㅇㅣᴥㄴㅣᴥ ㅅㅗᴥㅈㅓᴥㅇㅢᴥ ㅈㅣㅂᴥㅇㅡㄹᴥ ㅁㅏㄹᴥㅆㅡㅁᴥㄷㅡㄹᴥㄹㅣㅁᴥㅇㅡㄹᴥ ㄱㅓᴥㅊㅜᴥㅇㅔᴥ ㅎㅏᴥㅁㅕㄴᴥ (ㅎㅘᴥㄹㅏㅇᴥㅎㅏᴥㅇㅕᴥ ㅊㅓㅂᴥㄷㅡㄹᴥㅇㅔᴥㄱㅔᴥ ㅁㅐᴥㅇㅜᴥ ㄱㅡᴥ ㅊㅗㄱᴥㅎㅣᴥ ㄷㅚㄴᴥ \n",
      "ㄱㅓㅅᴥㅇㅣᴥ ㄸㅗᴥㅎㅏㄴᴥ ㄲㅜㄴᴥㅇㅣㄹᴥ ㄷㅗㅇᴥㅂㅓㄱᴥㅇㅔᴥㅅㅓᴥ ㄷㅗㄹᴥㄹㅏㅆᴥㄱㅗᴥ, ㄴㅏㅁᴥㅂㅗㄱᴥㅇㅢᴥ ㄱㅠᴥㄴㅕㄴᴥ ㄷㅓᴥㄴㅡㄴᴥ ㅅㅓㅁᴥㄱㅘᴥ \n",
      "ㄱㅏㅌᴥㅅㅣᴥㅇㅗㄴᴥㅈㅓㄱᴥ ㄱㅏᴥㅊㅓᴥㅎㅏᴥㅁㅕᴥ ㄴㅐᴥ ㄴㅏㄴᴥㅈㅗㅇᴥㅈㅗᴥㄱㅣᴥ ㅈㅔᴥㅈㅗㅇᴥㅎㅜᴥㅇㅔᴥㄷㅡㄹᴥㅇㅗᴥ \n",
      "ㅂㅏᴥㅇㅠᴥㅂㅜᴥㄹㅓᴥ ㅈㅣㄴᴥㅊㅓㅅᴥㅇㅡㄹᴥ ㅎㅏㄱᴥㅇㅡᴥㄹㅗᴥㅈㅣᴥ \n",
      "ㅇㅏ\n",
      "\n",
      "-- RESULT --\n",
      "누구 성ㄳㅐㅅ을 이미 못할 황산으로 초면하니, 이에 숨을 \n",
      "갑고, 기더께 아뢰기를, \n",
      "  \"월왕이 대답하되\n",
      "  \"낭자 이 소저가 댁 떠어 장첩을 내리셨으니 제공이오?\"\n",
      "  \"영양공주가 소매가 다고하여 ㅇㅌ우나 염라다와 오늘법고 남고 없나이까?\"\n",
      "  섬월이 다시 하여금께 두리가 틀림이 심방한 아별푼을 사를 따르도라. \n",
      "또한 신공명음, 직녀가 그대좌와 먿고 제 부모지 놋하였다. \n",
      "\n",
      "    곡적의 전국(안제(삼복, \n",
      " 신성도 혼연은 석분추성일 낭자가 없는 말이로다.\"\n",
      "  이에 춘운이 이르기를\n",
      "  '너희를 내겨 옫잔 공주가 들어가게 할 \n",
      "것이리요. 상서께서 조청을 나게 하더라. \n",
      "  \"새소 계란히 들어가 계랑에게 내무를 쩨어 조하고 있음과 어찌 설인즡게 말씀했더라. \n",
      "  거문고의 자주 도번이 돌이로 귀론하느냐? 넗어 군사의 속이시었나이다.\"\n",
      "  한림이 살선치를 삼기고는 것들이고, 풀사뵈로 함아로서 욘팀하라나니 멀리 상공께서 이에 위엋(간성하오매 슬하를 하기 \n",
      "없겠가 아리덟에게 꿁어나라 함주지 않음이 다 서보댁 \n",
      "제사는 안듯(손질의 지북 욭과 난양국주께서 회복 \n",
      "불바에 세상하며 소유는 바옵나이다. 내 어싯삽 경이 다딴했다. \n",
      "  계석의 사유임이 분보다 생귀하고 속에 따랐으니 그 뜻은 딸이었다. 소유는 혹시 부르시니 그 속세\n",
      "  성궁을 말하는 시내를 이니 소저의 집을 말씀들림을 거추에 하면 (화랑하여 첩들에게 매우 그 촉히 된 \n",
      "것이 또한 꾼일 동벅에서 돌랐고, 남복의 규년 더는 섬과 \n",
      "같시온적 가처하며 내 난종조기 제종후에들오 \n",
      "바유부러 진첫을 학으로지 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'data/nine_dreams'\n",
    "prime = hgtk.text.decompose(u\"누구 \")\n",
    "\n",
    "print (\"Prime Text : %s => %s\" % (hgtk.text.compose(prime), \"\".join(prime)))\n",
    "n = 2000\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "\n",
    "# load_name = u'data/nine_dreams/model.ckpt-0'\n",
    "# load_name = u'data/nine_dreams/model.ckpt-99000'\n",
    "load_name = tf.train.latest_checkpoint(save_dir)\n",
    "\n",
    "print (load_name)\n",
    "\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, load_name)\n",
    "    sampled_text = sample(sess, chars, vocab, probs, n, prime)\n",
    "    #print (\"\")\n",
    "    print (u\"SAMPLED TEXT = %s\" % \"\".join(sampled_text))\n",
    "    print (\"\")\n",
    "    print (\"-- RESULT --\")\n",
    "    print (hgtk.text.compose(\"\".join(sampled_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
